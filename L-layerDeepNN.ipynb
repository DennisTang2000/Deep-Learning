{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "training_images = np.zeros((87,576,432,3))\n",
    "test_set_images = np.zeros((23,576,432,3))\n",
    "train_set_labels = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "test_set_labels = np.array([[0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0]]) \n",
    "\n",
    "\n",
    "tempstr = \"\"\n",
    "for i in range(0,87):\n",
    "    tempstr = \"Training/\" + str(i+1) + \".jpeg\"\n",
    "    image = mpimg.imread(tempstr)\n",
    "    training_images[i] = image\n",
    "    \n",
    "for i in range(1, 24):\n",
    "    tempstr = \"Test/\" + str(i+1) + \".jpeg\"\n",
    "    image = mpimg.imread(tempstr)\n",
    "    test_set_images[i - 1] = image\n",
    "\n",
    "train_set_flatten = training_images.reshape(training_images.shape[0], -1).T\n",
    "test_set_flatten = test_set_images.reshape(test_set_images.shape[0], -1).T\n",
    "train_set = train_set_flatten/255\n",
    "\n",
    "print(\"done\")\n",
    "test_set = test_set_flatten/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [20, 10, 5, 3, 1]\n",
    "\n",
    "def initialize_parameters(layer_dims, X):\n",
    "    #layer_dims is an array that contains the number of neurons in each layer\n",
    "    #X is the training set. We need to use it to find the dimensions of W1\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = np.random.randn(layer_dims[0], X.shape[0]) * 0.01\n",
    "    #print(\"W1 shape \" + str(parameters[\"W1\"].shape))\n",
    "    parameters[\"b1\"] = np.random.randn(layer_dims[0], 1)  * 0.01\n",
    "    #print(\"b1 shape \" + str(parameters[\"b1\"].shape))\n",
    "    for l in range(1 , len(layer_dims)):\n",
    "        parameters[\"W\" + str(l + 1)] = np.random.randn(layer_dims[l], layer_dims[l - 1])* 0.01\n",
    "        #print(\"W\" + str(l  +  1) + \" shape \" + str(parameters[\"W\" + str(l + 1)].shape))\n",
    "        parameters[\"b\" + str(l + 1)] = np.random.randn(layer_dims[l], 1) * 0.01\n",
    "        #print(\"b\" + str(l  +  1) + \" shape \" + str(parameters[\"b\" + str(l + 1)].shape))\n",
    "    return parameters\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1/(1 + np.exp(-1*z))\n",
    "\n",
    "def activate(z, function):\n",
    "    if function == \"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "        \n",
    "    if function == \"tanh\":\n",
    "        return np.tanh(z)\n",
    "\n",
    "def forward_prop(parameters, layer_dims, X):\n",
    "    #calculates all Z in the method, then call a helper method to \n",
    "    #find A depending on the activation\n",
    "    #returns a vector with predicted values\n",
    "    #finding A requires knowing the activation function\n",
    "    #you must cache all Z values for GD in backprop\n",
    "    Z_cache = {}\n",
    "    A_cache = {}\n",
    "    layers = len(layer_dims) \n",
    "    A = X\n",
    "    A_cache[\"A0\"] = X\n",
    "    for i in range(1, layers ):\n",
    "        #i's last value is len(layer_dims) - 2\n",
    "        Z_cache[\"Z\" + str(i)] = np.dot(parameters[\"W\" + str(i)], A) + parameters[\"b\" + str(i)]\n",
    "        A = activate(Z_cache[\"Z\" + str(i)], \"tanh\")\n",
    "        A_cache[\"A\" + str(i)] = A\n",
    "        \n",
    "    Z_cache[\"Z\" + str(layers)] = np.dot(parameters[\"W\" + str(layers)], A_cache[\"A\" + str(len(layer_dims) - 1)]) + parameters[\"b\" + str(layers)]\n",
    "    A_cache[\"A\" + str(layers)] = activate(Z_cache[\"Z\" + str(layers)], \"sigmoid\")\n",
    "    \n",
    "    return Z_cache, A_cache\n",
    "\n",
    "\n",
    "def back_prop(Y, X, parameters, Z_cache, A_cache,  layer_dims):\n",
    "    #find all gradients dW, db, dA, dZ\n",
    "    #update all W, B parameters\n",
    "    grads = {}\n",
    "    layers = len(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    grads[\"dA\" + str(layers)] = - np.divide(Y, A_cache[\"A\" + str(layers)]) + np.divide(1 - Y, 1 -  A_cache[\"A\" + str(layers)]) \n",
    "    grads[\"dZ\" + str(layers)] = A_cache[\"A\" + str(layers)] - Y\n",
    "    for l in range(layers, 0, -1):\n",
    "        grads[\"db\" + str(l)] = 1/m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
    "        grads[\"dA\" + str(l - 1)] = np.dot(parameters[\"W\" + str(l)].T, grads[\"dZ\" + str(l)])\n",
    "        grads[\"dW\" + str(l)] = 1/m * np.dot(grads[\"dZ\" + str(l)], A_cache[\"A\" + str(l - 1)].T)\n",
    "        \n",
    "        if l > 1:\n",
    "            \n",
    "            grads[\"dZ\" + str(l - 1)] =  grads[\"dA\" + str(l-1)] * (1 - np.power(A_cache[\"A\" + str(l-1)], 2))\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def optimize(X, Y, Z_cache, A_cache, parameters, its, learning_rate, grads, layer_dims):\n",
    "    layers = len(layer_dims)\n",
    "    x = [1, 10, 25, 50, 100, 250, 500, 1000, 1750]\n",
    "    for i in range(its):\n",
    "        grads = back_prop(Y , X, parameters, Z_cache, A_cache, layer_dims)\n",
    "        for l in range(1, layers + 1):\n",
    "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)]  - learning_rate*grads[\"dW\" + str(l)]\n",
    "                \n",
    "            parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*grads[\"db\" + str(l)]\n",
    "        Z_cache, A_cache = forward_prop(parameters, layer_dims, X)\n",
    "        \n",
    "        if(i in x):\n",
    "            print(\"Completed \" + str(i) + \" iterations\")\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, parameters, layer_dims):\n",
    "    layers = len(layer_dims)\n",
    "    for i in range(1, layers):\n",
    "        if(i == 1):\n",
    "            Z = np.dot(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "            \n",
    "        else:\n",
    "            Z = np.dot(parameters[\"W\" + str(i)], A) + parameters[\"b\" + str(i)]\n",
    "                \n",
    "        print(\"Z\" + str(i) + \" \", Z)\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    \n",
    "    \n",
    "    Z = np.dot(parameters[\"W\" + str(layers)], A) + parameters[\"b\" + str(layers)]\n",
    "    print(\"W\" , parameters[\"W\" + str(layers)])\n",
    "    print(\"W shape\",  str(parameters[\"W\" + str(layers)].shape))\n",
    "    \n",
    "    print(\"final Z \" , Z)\n",
    "    \n",
    "    A = sigmoid(Z)\n",
    "    print(\"final A \" , A)\n",
    "    predict = np.array(A, copy = True)\n",
    "    predict[A < 0.75] = 0\n",
    "    predict[A >= 0.75] = 1\n",
    "    predict = np.array(predict)\n",
    "    print(\"predict array \", predict)\n",
    "    return predict\n",
    "            \n",
    "        \n",
    "def L_layer_deeplearning_model(layer_dims, Train, Train_labels, Test, Test_labels, its, learning_rate):\n",
    "    parameters = initialize_parameters(layer_dims, Train)\n",
    "    Z_cache, A_cache = forward_prop(parameters, layer_dims, Train)\n",
    "    \n",
    "    #print(\"Z1 shape \" + str(Z_cache[\"Z1\"].shape))\n",
    "    #print(\"Z2 shape \" + str(Z_cache[\"Z2\"].shape))\n",
    "    #print(\"A1 shape \" + str(A_cache[\"A1\"].shape))\n",
    "    #print(\"A2 shape \" + str(A_cache[\"A2\"].shape))\n",
    "    \n",
    "    grads = back_prop(Train_labels, Train, parameters, Z_cache, A_cache, layer_dims)\n",
    "    new_para = optimize(Train, Train_labels, Z_cache, A_cache, parameters, its, learning_rate, grads, layer_dims)\n",
    "    prediction = predict(Test, new_para, layer_dims)\n",
    "    \n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction - Test_labels)) * 100))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "layer_dims = [3, 5, 3, 1]  \n",
    "start = time.time()\n",
    "L_layer_deeplearning_model(layer_dims, train_set, train_set_labels, test_set, test_set_labels, 1500, 0.01 )\n",
    "end = time.time()\n",
    "print(\"time to run: \"  + str(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
